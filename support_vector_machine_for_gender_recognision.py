# -*- coding: utf-8 -*-
"""support-vector-machine-for-gender-recognision.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zz5Rr8e8eOU_Zasa2MJV2p7fBs_BJB2s
"""

# Jovian Commit Essentials
# Please retain and execute this cell without modifying the contents for `jovian.commit` to work
!pip install jovian --upgrade -q
import jovian
jovian.set_project('support-vector-machine-for-gender-recognision')
jovian.set_colab_id('1Zz5Rr8e8eOU_Zasa2MJV2p7fBs_BJB2s')

"""# **Gender Recognision using Support Vector Machine with Scikit Learn - Machine Learning**


Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems.

The ojective of SVM is to find a hyperplane in an N-Dimensional space to distinctly classify any datapoint.

This algorithm is commonly used when processing data with high dimensions

It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.

## Problem Statement

Here, we will process 3,168 recorded voice samples, collected from male and female speakers. The voice samples are pre-processed by acoustic analysis in R using the seewave and tuneR packages, with an analyzed frequency range of 0hz-280hz. Using this data, we will train our model to recognize the label of speech as Male or Female using SVM.
"""

!pip install jovian --upgrade --quiet

import jovian

# Execute this to save new versions of the notebook
jovian.commit(project="support-vector-machine-for-gender-recognision")

"""## Downloading the Data

In this section, we will download the data using `opendatasets` library

`opendatasets` is a Python library for downloading datasets from online sources like Kaggle and Google Drive using a simple Python command.
"""

!pip install opendatasets --upgrade --quiet

import opendatasets as od

dataset_url='https://www.kaggle.com/datasets/primaryobjects/voicegender?select=voice.csv'

od.download(dataset_url)

import os

"""Now, we will pass the path of the directory where the .csv file is downloaded."""

data_dir="./voicegender"

voice_csv=data_dir+'/voice.csv'

!pip install pandas --quiet

"""Now, we will convert the .csv file data to dataFrame using `pandas`"""

import pandas as pd

df=pd.read_csv(voice_csv)

df.head(10)

df.info()

df.describe()

"""## Exploratory Data Analysis and Visualization

Here, we will try to analyise the columns from the dataset and try to dervive its relation with the target column.

"""

!pip install plotly matplotlib seaborn --quiet

# Commented out IPython magic to ensure Python compatibility.
import plotly.express as px
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

sns.set_style('darkgrid')
matplotlib.rcParams['font.size'] = 14
matplotlib.rcParams['figure.figsize'] = (10, 6)
matplotlib.rcParams['figure.facecolor'] = '#00000000'

df.corr()

px.histogram(df,x='modindx',y='centroid',color='label', title='Modulation Idex vs Centroid')

"""From the above diagram, we can see that maximum female voice lies in the range of 0.07 to 0.2 for Modulation Index.

"""

plt.figure(figsize=(20,10))
corr=df.corr()
sns.heatmap(corr,annot=True, cmap=plt.cm.Reds)
plt.show()

"""From the above chart , we can see that Centroid, Median and standard Deviation has maximum correlation with other columns."""



df.head(4)

px.scatter(df, x='meanfreq',y='sd', color='label', title='Mean VS Sandard Deviation')

"""From the above chart, we can see that the labels are clearly divided into 2 groups. Females having standard deviaition in the range of 0.02 to 0.06 and Male in the range of 0.05 to 0.08"""

px.scatter(df, x='Q25',y='Q75', color='label',size='IQR', title = "Q25 VS Q75")

"""from the above chart, we can see that

## Training, Validation and Test Sets

While building real-world machine learning models, it is quite common to split the dataset into three parts:

Training set - used to train the model, i.e., compute the loss and adjust the model's weights using an optimization technique.

Validation set - used to evaluate the model during training, tune model hyperparameters (optimization technique, regularization etc.), and pick the best version of the model. Picking a good validation set is essential for training models that generalize well. Learn more here.

Test set - used to compare different models or approaches and report the model's final accuracy. For many datasets, test sets are provided separately. The test set should reflect the kind of data the model will encounter in the real-world, as closely as feasible.


As a general rule of thumb you can use around 60% of the data for the training set, 20% for the validation set and 20% for the test set. If a separate test set is already provided, you can use a 75%-25% training-validation split.

<img src="https://i.imgur.com/j8eITrK.png" width="480">

When rows in the dataset have no inherent order, it's common practice to pick random subsets of rows for creating test and validation sets. This can be done using the train_test_split utility from scikit-learn. Learn more about it here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
"""

!pip install scikit-learn --upgarde --quiet

"""we will use the `train_test_split` library to split the data in desired sections"""

from sklearn.model_selection import train_test_split

train_val_df, test_df=train_test_split(df, test_size=0.2, random_state=42)
train_df,val_df=train_test_split(train_val_df,test_size=0.25,random_state=42)

print('train_df.shape :', train_df.shape)
print('val_df.shape :', val_df.shape)
print('test_df.shape :', test_df.shape)

"""## Identifying Input and Target Columns

Here, we will identify the target column from the entire dataset provided and create name dataframes that will contain the input columns and target data columns.
"""

input_columns=list(df.columns[1:-1])

input_columns

target_column='label'

"""We will create new dataframes with input columns and target columns for training data, Validation data and test data."""

train_inputs=train_df[input_columns].copy()
train_target=train_df[target_column].copy()

val_inputs=val_df[input_columns].copy()
val_target=val_df[target_column].copy()

test_inputs=test_df[input_columns].copy()
test_target=test_df[target_column].copy()

"""Now, we will check for the columns having non numeric data to convert it into numeric values.

"""

df.info()

"""Here all the columns are numeric so we can exclude the process of One hot encoding.

We will check the data for any null values in the data to handle it because training the data.
"""

df[input_columns].isna().sum()

"""here, there are not null values in the data.

## Scaling Numeric Features

Here, we will scale numeric features to ensure that no particular feature has a disproportionate impact on the model's loss. 

We will convert the data in the range of (0,1)

We will use `MinMaxScaler` library to scale the data.
"""

from sklearn.preprocessing import MinMaxScaler

scaler=MinMaxScaler()

scaler.fit(df[input_columns])



train_inputs[input_columns] = scaler.transform(train_inputs[input_columns])
val_inputs[input_columns] = scaler.transform(val_inputs[input_columns])
test_inputs[input_columns] = scaler.transform(test_inputs[input_columns])

"""Let's check the data now."""

train_inputs.describe()

"""We can see that all the data from numeric columns has been scaled in the range of (0,1)

Since no categorical column, One Hot Encoding is not needed

## Training an SVM Model

We will use the `SVC` library to train the SVM model.

Here, we will train the model with kernel Variation as linear and C = 1

A kernel is a function used in SVM for helping to solve problems. They provide shortcuts to avoid complex calculations. 

C is a hypermeter which is set before the training model and used to control error.
"""



from sklearn.svm import SVC
model=SVC(C=1,kernel='linear')

"""Now, we will fit the data in the above model created."""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model.fit(train_inputs, train_target)

"""Now, we will check the accuracy score of the above fitted data using `model.score()`"""

model.score(val_inputs,val_target)

"""The Validation dataset shows an accuracy of 96%. We will now test the above model with training dataset.

Now, we will check the accuracy score for test dataset.
"""

model.score(test_inputs,test_target)

"""The testing data predictions

Here, the accuracy of test dataaset is 98.1%. Since the accuracy is already is 98%, we can stop at this point avoid the process of Hypertuning the parameters.
"""











jovian.commit()